# - threshold [input]
# - s.mean: rolling mean
# - s.sd: rolling standard deviation (s.d.)
# - anomalies: anomalous data points, as a subset of s
# TODO: Compute rolling mean
# Hint: use rollapply() with align = 'right' and fill = 'extend'
s.mean = rollapply(s, window_size, mean, align='right', fill='extend')
# TODO: Compute rolling standard deviation
s.sd = rollapply(s, window_size, sd, align='right', fill='extend')
# TODO: Find anomalies
# Hint: Look for data points that are more than (threshold * s.d.) away from mean
s.upper = s.mean + threshold * (s.sd)
s.lower = s.mean - threshold * (s.sd)
s.anomalies = ifelse(s >= s.upper | s <= s.lower, 'yes', 'no')
# TODO: Filter anomalies to only keep extrema
# Hint: Look for peaks and troughs
anomalies = s[s.anomalies == 'yes']
# TODO(optional): Further filtering to reduce duplicates and false positives
# Return results as a named list (include input params as well)
return(list(s=s, window_size=window_size, threshold=threshold,
s.mean=s.mean, s.sd=s.sd, anomalies=anomalies))
}
analyze <- function(csv_filename, window_days=3, threshold=4) {
# Analyze a time series, looking for anomalies.
#
# Params:
# - csv_filename: CSV file with two columns: timestamp, value
# - window_days: no. of days to include in moving window
# - threshold: parameter passed on to find_anomalies()
#
# Returns:
# - s: results returned by find_anomalies()
s <- load_ts(csv_filename)  # load time series data from CSV file
# Compute samples per day to set rolling window size
avg_delta <- difftime(index(s)[length(s)], index(s)[1], units='secs') / length(s)
samples_per_day <- 24 * 60 * 60 / as.numeric(avg_delta)
window_size <- as.integer(window_days * samples_per_day)  # no. of days * samples_per_day
# Find anomalies
res <- find_anomalies(s, window_size, threshold)
cat(paste(csv_filename, ": window_size = ", window_size, ", threshold = ", threshold, sep=""), end="\n")
cat(length(res$anomalies), "anomalies found", end="\n")
#print(res$anomalies)
# Pass on results returned by find_anomalies()
return(res)
}
visualize <- function(res, wins=NA, title="Anomaly Detection Results") {
# Visualize the results of anomaly detection.
#
# Params:
# - res: anomaly detection results, as returned by find_anomalies()
# - wins: optional windows to be highlighted
# - title: main title for the plot
#
# Returns: Nothing
# Create a new plot
plot.new()
# Plot original time series
if(!is.na(wins) && nrow(wins) > 0) {
plot(res$s, type="n", main=title)  # create a blank plot first
print(lines(res$s))  # then draw the time series
} else {
plot(res$s, main=title)
}
# TODO: Show moving average
print(lines(res$s.mean, col='blue'))
# TODO: Draw margins at mean +/- (threshold * s.d.)
s.upper = res$s.mean + res$threshold * (res$s.sd)
s.lower = res$s.mean - res$threshold * (res$s.sd)
print(lines(s.upper, col='orange'))
print(lines(s.lower, col='orange'))
# TODO: Mark anomalies
print(points(res$anomalies, col='red', cex=1))
# Optional highlight windows
if(!is.na(wins) && nrow(wins) > 0) {
rect(wins$beg, min(res$s), wins$end, max(res$s), col="#CCCCEE77", border=NA)  # add highlights
}
}
# NOTE: Do not put any code outside the functions or the following "main" block
if(getOption("run.main", default=TRUE)) {
# Analyze
csv_filename <- "realAWSCloudwatch/ec2_cpu_utilization_5f5533.csv"
res <- analyze(paste(data_dir, csv_filename, sep="/"), window_days=3, threshold=4)
# Visualize (with ground truth windows highlighted)
wins <- read.csv(paste(label_dir, csv_filename, sep="/"), stringsAsFactors=FALSE)  # ground truth windows
wins$beg <- as.POSIXct(wins$beg)  # convert to POSIX datetime
wins$end <- as.POSIXct(wins$end)
visualize(res, wins=wins, title=paste("Anomaly Detection Results", csv_filename, sep="\n"))
}
# Activity: Time Series Analysis
#title: "CSE 6242 Assignment 3"
#author: Vincent La (Georgia Tech ID - vla6)
#date: November 5, 2017
library(zoo)  # basic time series package
library(xts)  # eXtensible Time Series package
data_dir <- "data"
label_dir <- "labeled_windows"
load_ts <- function(csv_filename) {
# Load and return time series data from a CSV file.
#
# Params:
# - csv_filename: CSV file with two columns: timestamp, value
#
# Returns:
# - s: time series data of type xts
df <- read.csv(csv_filename, stringsAsFactors=FALSE)
# TODO: convert timestamp column to POSIX datetime
df$timestamp = as.POSIXct(df$timestamp)
# TODO: create xts time series from dataframe
s = xts(df$value, order.by=df$timestamp)
return(s)  # return time series
}
find_anomalies <- function(s, window_size=11, threshold=4) {
# Find anomalous data points in a time series.
#
# Params:
# - s: time series data, as returned by load_ts()
# - window_size: size of window used to compute rolling statistics
# - threshold: parameter used to identify outliers
#
# Returns: A list with the following named items:
# - s [input]
# - window_size [input]
# - threshold [input]
# - s.mean: rolling mean
# - s.sd: rolling standard deviation (s.d.)
# - anomalies: anomalous data points, as a subset of s
# TODO: Compute rolling mean
# Hint: use rollapply() with align = 'right' and fill = 'extend'
s.mean = rollapply(s, window_size, mean, align='right', fill='extend')
# TODO: Compute rolling standard deviation
s.sd = rollapply(s, window_size, sd, align='right', fill='extend')
# TODO: Find anomalies
# Hint: Look for data points that are more than (threshold * s.d.) away from mean
s.upper = s.mean + threshold * (s.sd)
s.lower = s.mean - threshold * (s.sd)
s.anomalies = ifelse(s >= s.upper | s <= s.lower, 'yes', 'no')
# TODO: Filter anomalies to only keep extrema
# Hint: Look for peaks and troughs
anomalies = s[s.anomalies == 'yes']
# TODO(optional): Further filtering to reduce duplicates and false positives
# Return results as a named list (include input params as well)
return(list(s=s, window_size=window_size, threshold=threshold,
s.mean=s.mean, s.sd=s.sd, anomalies=anomalies))
}
analyze <- function(csv_filename, window_days=3, threshold=4) {
# Analyze a time series, looking for anomalies.
#
# Params:
# - csv_filename: CSV file with two columns: timestamp, value
# - window_days: no. of days to include in moving window
# - threshold: parameter passed on to find_anomalies()
#
# Returns:
# - s: results returned by find_anomalies()
s <- load_ts(csv_filename)  # load time series data from CSV file
# Compute samples per day to set rolling window size
avg_delta <- difftime(index(s)[length(s)], index(s)[1], units='secs') / length(s)
samples_per_day <- 24 * 60 * 60 / as.numeric(avg_delta)
window_size <- as.integer(window_days * samples_per_day)  # no. of days * samples_per_day
# Find anomalies
res <- find_anomalies(s, window_size, threshold)
cat(paste(csv_filename, ": window_size = ", window_size, ", threshold = ", threshold, sep=""), end="\n")
cat(length(res$anomalies), "anomalies found", end="\n")
#print(res$anomalies)
# Pass on results returned by find_anomalies()
return(res)
}
visualize <- function(res, wins=NA, title="Anomaly Detection Results") {
# Visualize the results of anomaly detection.
#
# Params:
# - res: anomaly detection results, as returned by find_anomalies()
# - wins: optional windows to be highlighted
# - title: main title for the plot
#
# Returns: Nothing
# Create a new plot
plot.new()
# Plot original time series
if(!is.na(wins) && nrow(wins) > 0) {
plot(res$s, type="n", main=title)  # create a blank plot first
print(lines(res$s))  # then draw the time series
} else {
plot(res$s, main=title)
}
# TODO: Show moving average
print(lines(res$s.mean, col='blue'))
# TODO: Draw margins at mean +/- (threshold * s.d.)
s.upper = res$s.mean + res$threshold * (res$s.sd)
s.lower = res$s.mean - res$threshold * (res$s.sd)
print(lines(s.upper, col='orange'))
print(lines(s.lower, col='orange'))
# TODO: Mark anomalies
print(points(res$anomalies, col='red', cex=2))
# Optional highlight windows
if(!is.na(wins) && nrow(wins) > 0) {
rect(wins$beg, min(res$s), wins$end, max(res$s), col="#CCCCEE77", border=NA)  # add highlights
}
}
# NOTE: Do not put any code outside the functions or the following "main" block
if(getOption("run.main", default=TRUE)) {
# Analyze
csv_filename <- "realAWSCloudwatch/ec2_cpu_utilization_5f5533.csv"
res <- analyze(paste(data_dir, csv_filename, sep="/"), window_days=3, threshold=4)
# Visualize (with ground truth windows highlighted)
wins <- read.csv(paste(label_dir, csv_filename, sep="/"), stringsAsFactors=FALSE)  # ground truth windows
wins$beg <- as.POSIXct(wins$beg)  # convert to POSIX datetime
wins$end <- as.POSIXct(wins$end)
visualize(res, wins=wins, title=paste("Anomaly Detection Results", csv_filename, sep="\n"))
}
# Activity: Time Series Analysis
#title: "CSE 6242 Assignment 3"
#author: Vincent La (Georgia Tech ID - vla6)
#date: November 5, 2017
library(zoo)  # basic time series package
library(xts)  # eXtensible Time Series package
data_dir <- "data"
label_dir <- "labeled_windows"
load_ts <- function(csv_filename) {
# Load and return time series data from a CSV file.
#
# Params:
# - csv_filename: CSV file with two columns: timestamp, value
#
# Returns:
# - s: time series data of type xts
df <- read.csv(csv_filename, stringsAsFactors=FALSE)
# TODO: convert timestamp column to POSIX datetime
df$timestamp = as.POSIXct(df$timestamp)
# TODO: create xts time series from dataframe
s = xts(df$value, order.by=df$timestamp)
return(s)  # return time series
}
find_anomalies <- function(s, window_size=11, threshold=4) {
# Find anomalous data points in a time series.
#
# Params:
# - s: time series data, as returned by load_ts()
# - window_size: size of window used to compute rolling statistics
# - threshold: parameter used to identify outliers
#
# Returns: A list with the following named items:
# - s [input]
# - window_size [input]
# - threshold [input]
# - s.mean: rolling mean
# - s.sd: rolling standard deviation (s.d.)
# - anomalies: anomalous data points, as a subset of s
# TODO: Compute rolling mean
# Hint: use rollapply() with align = 'right' and fill = 'extend'
s.mean = rollapply(s, window_size, mean, align='right', fill='extend')
# TODO: Compute rolling standard deviation
s.sd = rollapply(s, window_size, sd, align='right', fill='extend')
# TODO: Find anomalies
# Hint: Look for data points that are more than (threshold * s.d.) away from mean
s.upper = s.mean + threshold * (s.sd)
s.lower = s.mean - threshold * (s.sd)
s.anomalies = ifelse(s >= s.upper | s <= s.lower, 'yes', 'no')
# TODO: Filter anomalies to only keep extrema
# Hint: Look for peaks and troughs
anomalies = s[s.anomalies == 'yes']
# TODO(optional): Further filtering to reduce duplicates and false positives
# Return results as a named list (include input params as well)
return(list(s=s, window_size=window_size, threshold=threshold,
s.mean=s.mean, s.sd=s.sd, anomalies=anomalies))
}
analyze <- function(csv_filename, window_days=3, threshold=4) {
# Analyze a time series, looking for anomalies.
#
# Params:
# - csv_filename: CSV file with two columns: timestamp, value
# - window_days: no. of days to include in moving window
# - threshold: parameter passed on to find_anomalies()
#
# Returns:
# - s: results returned by find_anomalies()
s <- load_ts(csv_filename)  # load time series data from CSV file
# Compute samples per day to set rolling window size
avg_delta <- difftime(index(s)[length(s)], index(s)[1], units='secs') / length(s)
samples_per_day <- 24 * 60 * 60 / as.numeric(avg_delta)
window_size <- as.integer(window_days * samples_per_day)  # no. of days * samples_per_day
# Find anomalies
res <- find_anomalies(s, window_size, threshold)
cat(paste(csv_filename, ": window_size = ", window_size, ", threshold = ", threshold, sep=""), end="\n")
cat(length(res$anomalies), "anomalies found", end="\n")
#print(res$anomalies)
# Pass on results returned by find_anomalies()
return(res)
}
visualize <- function(res, wins=NA, title="Anomaly Detection Results") {
# Visualize the results of anomaly detection.
#
# Params:
# - res: anomaly detection results, as returned by find_anomalies()
# - wins: optional windows to be highlighted
# - title: main title for the plot
#
# Returns: Nothing
# Create a new plot
plot.new()
# Plot original time series
if(!is.na(wins) && nrow(wins) > 0) {
plot(res$s, type="n", main=title)  # create a blank plot first
print(lines(res$s))  # then draw the time series
} else {
plot(res$s, main=title)
}
# TODO: Show moving average
print(lines(res$s.mean, col='blue'))
# TODO: Draw margins at mean +/- (threshold * s.d.)
s.upper = res$s.mean + res$threshold * (res$s.sd)
s.lower = res$s.mean - res$threshold * (res$s.sd)
print(lines(s.upper, col='orange'))
print(lines(s.lower, col='orange'))
# TODO: Mark anomalies
print(points(res$anomalies, col='red', pch=20, cex=5))
# Optional highlight windows
if(!is.na(wins) && nrow(wins) > 0) {
rect(wins$beg, min(res$s), wins$end, max(res$s), col="#CCCCEE77", border=NA)  # add highlights
}
}
# NOTE: Do not put any code outside the functions or the following "main" block
if(getOption("run.main", default=TRUE)) {
# Analyze
csv_filename <- "realAWSCloudwatch/ec2_cpu_utilization_5f5533.csv"
res <- analyze(paste(data_dir, csv_filename, sep="/"), window_days=3, threshold=4)
# Visualize (with ground truth windows highlighted)
wins <- read.csv(paste(label_dir, csv_filename, sep="/"), stringsAsFactors=FALSE)  # ground truth windows
wins$beg <- as.POSIXct(wins$beg)  # convert to POSIX datetime
wins$end <- as.POSIXct(wins$end)
visualize(res, wins=wins, title=paste("Anomaly Detection Results", csv_filename, sep="\n"))
}
# Activity: Time Series Analysis - Unit Tests
library(testthat)
context("Time Series Analysis")  # set a context for testing
options(run.main=FALSE)  # suppress running "main" block
source("ac4.R")  # load functions to be tested
options(run.main=TRUE)  # revert back to running "main" block
# Define common variables and utility functions
data_dir <- "data"
label_dir <- "labeled_windows"
# Make sure load() works as expected
test_that("load_ts() returns a time series", {
csv_filename <- "realAWSCloudwatch/ec2_cpu_utilization_5f5533.csv"
expect_silent(s <- load_ts(paste(data_dir, csv_filename, sep="/")))
expect_is(s, "xts")
expect_length(s, 4032)
})
# Make sure find_anomalies() works as expected
test_that("find_anomalies() returns valid results", {
csv_filename <- "realAWSCloudwatch/ec2_cpu_utilization_5f5533.csv"
window_size <- 864  # 3 days * samples_per_day
threshold <- 4  # 4 s.d. away from mean
s <- load_ts(paste(data_dir, csv_filename, sep="/"))  # load data (will fail if load_ts isn't good)
expect_silent(res <- find_anomalies(s, window_size=window_size, threshold=threshold))
expect_named(res, c("s", "window_size", "threshold", "s.mean", "s.sd", "anomalies"), ignore.order=TRUE)
expect_equal(res$s, s)
expect_equal(res$window_size, window_size)
expect_equal(res$threshold, threshold)
expect_equal(length(res$s.mean), length(s))
expect_equal(length(res$s.sd), length(s))
})
# Finally, run find_anomalies() on a few test inputs and check for approximate match
debug <- FALSE  # turn on debug to print diagnostic values instead of running actual assertions
test_cases <- rbind.data.frame(
data.frame(csv_filename = "realAWSCloudwatch/ec2_cpu_utilization_5f5533.csv", window_size = 800, threshold = 3.5),
data.frame(csv_filename = "realAWSCloudwatch/ec2_network_in_257a54.csv", window_size = 1200, threshold = 8),
data.frame(csv_filename = "realTraffic/speed_6005.csv", window_size = 800, threshold = 6.5)
)
if(debug) {
cat("[DEBUG] Test cases:", end="\n")
print(test_cases)
}
for(t in 1:nrow(test_cases)) {
# Prepare test case
csv_filename <- test_cases$csv_filename[[t]]
window_size <- test_cases$window_size[[t]]
threshold <- test_cases$threshold[[t]]
s <- load_ts(paste(data_dir, csv_filename, sep="/"))  # load data (will fail if load_ts isn't good)
w <- read.csv(paste(label_dir, csv_filename, sep="/"), stringsAsFactors=FALSE)  # ground truth windows
w$beg <- as.POSIXct(w$beg)
w$end <- as.POSIXct(w$end)
test_that(paste("find_anomalies() in", csv_filename), {
res <- find_anomalies(s, window_size=window_size, threshold=threshold)
# Find matches in ground-truth windows
num_reported <- length(res$anomalies)
num_false <- 0  # false positives (i.e. outside any window)
hits <- integer(nrow(w))  # detections within each window
if(nrow(w) > 0 && num_reported > 0) {
for(i in 1:num_reported) {
timestamp <- index(res$anomalies[i])
matched <- FALSE
for(j in 1:nrow(w)) {
if(w$beg[j] <= timestamp & timestamp <= w$end[j]) {
hits[j] <- hits[j] + 1
matched <- TRUE
break
}
}
if(!matched) {
num_false <- num_false + 1
}
}
}
num_hits <- sum(hits)  # total hits
num_unique <- sum(hits > 0)  # unique hits (i.e. one per detected window)
num_dupes <- num_hits - num_unique  # duplicate hits
num_missed <- nrow(w) - num_unique  # windows completely missed
# Check if performance is within expected bounds
if(debug) {
cat(paste("[DEBUG] find_anomalies() in", csv_filename), end="\n")
cat(paste(nrow(w), " true anomaly windows; ", num_reported, " anomalies reported: ",
num_unique, " unique, ", num_dupes, " duplicate, ", num_false, " false positive(s), ",
num_missed, " missed", sep=""), end="\n")  # [debug]
visualize(res, wins=w, title=paste("Anomaly Detection Results", csv_filename, sep="\n"))
} else {
expect_gte(num_unique, 0.75 * nrow(w), label="Accuracy", expected.label="75%")  # accuracy is at least 75%
expect_lte(num_false, 0.5 * num_reported, label="False positive rate", expected.label="50%")  # false positive rate is at most 50%
expect_lte(num_dupes, 0.75 * num_reported, label="Duplicate rate", expected.label="75%")  # duplicate rate is at most 75%
}
})
}
# Activity: Time Series Analysis - Unit Tests
library(testthat)
context("Time Series Analysis")  # set a context for testing
options(run.main=FALSE)  # suppress running "main" block
source("ac4.R")  # load functions to be tested
options(run.main=TRUE)  # revert back to running "main" block
# Define common variables and utility functions
data_dir <- "data"
label_dir <- "labeled_windows"
# Make sure load() works as expected
test_that("load_ts() returns a time series", {
csv_filename <- "realAWSCloudwatch/ec2_cpu_utilization_5f5533.csv"
expect_silent(s <- load_ts(paste(data_dir, csv_filename, sep="/")))
expect_is(s, "xts")
expect_length(s, 4032)
})
# Make sure find_anomalies() works as expected
test_that("find_anomalies() returns valid results", {
csv_filename <- "realAWSCloudwatch/ec2_cpu_utilization_5f5533.csv"
window_size <- 864  # 3 days * samples_per_day
threshold <- 4  # 4 s.d. away from mean
s <- load_ts(paste(data_dir, csv_filename, sep="/"))  # load data (will fail if load_ts isn't good)
expect_silent(res <- find_anomalies(s, window_size=window_size, threshold=threshold))
expect_named(res, c("s", "window_size", "threshold", "s.mean", "s.sd", "anomalies"), ignore.order=TRUE)
expect_equal(res$s, s)
expect_equal(res$window_size, window_size)
expect_equal(res$threshold, threshold)
expect_equal(length(res$s.mean), length(s))
expect_equal(length(res$s.sd), length(s))
})
# Finally, run find_anomalies() on a few test inputs and check for approximate match
debug <- FALSE  # turn on debug to print diagnostic values instead of running actual assertions
test_cases <- rbind.data.frame(
data.frame(csv_filename = "realAWSCloudwatch/ec2_cpu_utilization_5f5533.csv", window_size = 800, threshold = 3.5),
data.frame(csv_filename = "realAWSCloudwatch/ec2_network_in_257a54.csv", window_size = 1200, threshold = 8),
data.frame(csv_filename = "realTraffic/speed_6005.csv", window_size = 800, threshold = 6.5)
)
if(debug) {
cat("[DEBUG] Test cases:", end="\n")
print(test_cases)
}
for(t in 1:nrow(test_cases)) {
# Prepare test case
csv_filename <- test_cases$csv_filename[[t]]
window_size <- test_cases$window_size[[t]]
threshold <- test_cases$threshold[[t]]
s <- load_ts(paste(data_dir, csv_filename, sep="/"))  # load data (will fail if load_ts isn't good)
w <- read.csv(paste(label_dir, csv_filename, sep="/"), stringsAsFactors=FALSE)  # ground truth windows
w$beg <- as.POSIXct(w$beg)
w$end <- as.POSIXct(w$end)
test_that(paste("find_anomalies() in", csv_filename), {
res <- find_anomalies(s, window_size=window_size, threshold=threshold)
# Find matches in ground-truth windows
num_reported <- length(res$anomalies)
num_false <- 0  # false positives (i.e. outside any window)
hits <- integer(nrow(w))  # detections within each window
if(nrow(w) > 0 && num_reported > 0) {
for(i in 1:num_reported) {
timestamp <- index(res$anomalies[i])
matched <- FALSE
for(j in 1:nrow(w)) {
if(w$beg[j] <= timestamp & timestamp <= w$end[j]) {
hits[j] <- hits[j] + 1
matched <- TRUE
break
}
}
if(!matched) {
num_false <- num_false + 1
}
}
}
num_hits <- sum(hits)  # total hits
num_unique <- sum(hits > 0)  # unique hits (i.e. one per detected window)
num_dupes <- num_hits - num_unique  # duplicate hits
num_missed <- nrow(w) - num_unique  # windows completely missed
# Check if performance is within expected bounds
if(debug) {
cat(paste("[DEBUG] find_anomalies() in", csv_filename), end="\n")
cat(paste(nrow(w), " true anomaly windows; ", num_reported, " anomalies reported: ",
num_unique, " unique, ", num_dupes, " duplicate, ", num_false, " false positive(s), ",
num_missed, " missed", sep=""), end="\n")  # [debug]
visualize(res, wins=w, title=paste("Anomaly Detection Results", csv_filename, sep="\n"))
} else {
expect_gte(num_unique, 0.75 * nrow(w), label="Accuracy", expected.label="75%")  # accuracy is at least 75%
expect_lte(num_false, 0.5 * num_reported, label="False positive rate", expected.label="50%")  # false positive rate is at most 50%
expect_lte(num_dupes, 0.75 * num_reported, label="Duplicate rate", expected.label="75%")  # duplicate rate is at most 75%
}
})
}
